{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "violent-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "infinite-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 4 3 3 3 7 7 7 7 0 0 0 6 6 6 5 5 5 5 5 5 5 2 2 2 2 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "emotional-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "uniform-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_len = 20\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "little-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 16)            16000     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 16,680\n",
      "Trainable params: 16,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "33/33 [==============================] - 0s 4ms/sample - loss: 2.0781 - acc: 0.0909\n",
      "Epoch 2/500\n",
      "33/33 [==============================] - 0s 112us/sample - loss: 2.0765 - acc: 0.2121\n",
      "Epoch 3/500\n",
      "33/33 [==============================] - 0s 170us/sample - loss: 2.0754 - acc: 0.2121\n",
      "Epoch 4/500\n",
      "33/33 [==============================] - 0s 136us/sample - loss: 2.0745 - acc: 0.2121\n",
      "Epoch 5/500\n",
      "33/33 [==============================] - 0s 133us/sample - loss: 2.0736 - acc: 0.2121\n",
      "Epoch 6/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 2.0729 - acc: 0.2121\n",
      "Epoch 7/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 2.0724 - acc: 0.2121\n",
      "Epoch 8/500\n",
      "33/33 [==============================] - 0s 69us/sample - loss: 2.0719 - acc: 0.2121\n",
      "Epoch 9/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 2.0715 - acc: 0.2121\n",
      "Epoch 10/500\n",
      "33/33 [==============================] - 0s 134us/sample - loss: 2.0710 - acc: 0.2121\n",
      "Epoch 11/500\n",
      "33/33 [==============================] - 0s 181us/sample - loss: 2.0705 - acc: 0.2121\n",
      "Epoch 12/500\n",
      "33/33 [==============================] - 0s 194us/sample - loss: 2.0698 - acc: 0.2121\n",
      "Epoch 13/500\n",
      "33/33 [==============================] - 0s 181us/sample - loss: 2.0692 - acc: 0.2121\n",
      "Epoch 14/500\n",
      "33/33 [==============================] - 0s 271us/sample - loss: 2.0686 - acc: 0.2121\n",
      "Epoch 15/500\n",
      "33/33 [==============================] - 0s 115us/sample - loss: 2.0680 - acc: 0.2121\n",
      "Epoch 16/500\n",
      "33/33 [==============================] - 0s 288us/sample - loss: 2.0674 - acc: 0.2121\n",
      "Epoch 17/500\n",
      "33/33 [==============================] - 0s 216us/sample - loss: 2.0667 - acc: 0.2121\n",
      "Epoch 18/500\n",
      "33/33 [==============================] - 0s 193us/sample - loss: 2.0660 - acc: 0.2121\n",
      "Epoch 19/500\n",
      "33/33 [==============================] - 0s 281us/sample - loss: 2.0653 - acc: 0.2121\n",
      "Epoch 20/500\n",
      "33/33 [==============================] - 0s 265us/sample - loss: 2.0645 - acc: 0.2121\n",
      "Epoch 21/500\n",
      "33/33 [==============================] - 0s 171us/sample - loss: 2.0638 - acc: 0.2121\n",
      "Epoch 22/500\n",
      "33/33 [==============================] - 0s 243us/sample - loss: 2.0631 - acc: 0.2121\n",
      "Epoch 23/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 2.0623 - acc: 0.2121\n",
      "Epoch 24/500\n",
      "33/33 [==============================] - 0s 245us/sample - loss: 2.0615 - acc: 0.2121\n",
      "Epoch 25/500\n",
      "33/33 [==============================] - 0s 227us/sample - loss: 2.0606 - acc: 0.2121\n",
      "Epoch 26/500\n",
      "33/33 [==============================] - 0s 98us/sample - loss: 2.0598 - acc: 0.2121\n",
      "Epoch 27/500\n",
      "33/33 [==============================] - 0s 165us/sample - loss: 2.0589 - acc: 0.2121\n",
      "Epoch 28/500\n",
      "33/33 [==============================] - 0s 158us/sample - loss: 2.0582 - acc: 0.2121\n",
      "Epoch 29/500\n",
      "33/33 [==============================] - 0s 175us/sample - loss: 2.0574 - acc: 0.2121\n",
      "Epoch 30/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 2.0564 - acc: 0.2121\n",
      "Epoch 31/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 2.0556 - acc: 0.2121\n",
      "Epoch 32/500\n",
      "33/33 [==============================] - 0s 157us/sample - loss: 2.0546 - acc: 0.2121\n",
      "Epoch 33/500\n",
      "33/33 [==============================] - 0s 201us/sample - loss: 2.0538 - acc: 0.2121\n",
      "Epoch 34/500\n",
      "33/33 [==============================] - 0s 238us/sample - loss: 2.0532 - acc: 0.2121\n",
      "Epoch 35/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 2.0525 - acc: 0.2121\n",
      "Epoch 36/500\n",
      "33/33 [==============================] - 0s 196us/sample - loss: 2.0521 - acc: 0.2121\n",
      "Epoch 37/500\n",
      "33/33 [==============================] - 0s 174us/sample - loss: 2.0516 - acc: 0.2121\n",
      "Epoch 38/500\n",
      "33/33 [==============================] - 0s 255us/sample - loss: 2.0510 - acc: 0.2121\n",
      "Epoch 39/500\n",
      "33/33 [==============================] - 0s 237us/sample - loss: 2.0504 - acc: 0.2121\n",
      "Epoch 40/500\n",
      "33/33 [==============================] - 0s 124us/sample - loss: 2.0498 - acc: 0.2121\n",
      "Epoch 41/500\n",
      "33/33 [==============================] - 0s 274us/sample - loss: 2.0493 - acc: 0.2121\n",
      "Epoch 42/500\n",
      "33/33 [==============================] - 0s 278us/sample - loss: 2.0488 - acc: 0.2121\n",
      "Epoch 43/500\n",
      "33/33 [==============================] - 0s 143us/sample - loss: 2.0485 - acc: 0.2121\n",
      "Epoch 44/500\n",
      "33/33 [==============================] - 0s 236us/sample - loss: 2.0482 - acc: 0.2121\n",
      "Epoch 45/500\n",
      "33/33 [==============================] - 0s 201us/sample - loss: 2.0476 - acc: 0.2121\n",
      "Epoch 46/500\n",
      "33/33 [==============================] - 0s 150us/sample - loss: 2.0473 - acc: 0.2121\n",
      "Epoch 47/500\n",
      "33/33 [==============================] - 0s 344us/sample - loss: 2.0468 - acc: 0.2121\n",
      "Epoch 48/500\n",
      "33/33 [==============================] - 0s 131us/sample - loss: 2.0463 - acc: 0.2121\n",
      "Epoch 49/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 2.0457 - acc: 0.2121\n",
      "Epoch 50/500\n",
      "33/33 [==============================] - 0s 257us/sample - loss: 2.0449 - acc: 0.2121\n",
      "Epoch 51/500\n",
      "33/33 [==============================] - 0s 259us/sample - loss: 2.0441 - acc: 0.2121\n",
      "Epoch 52/500\n",
      "33/33 [==============================] - 0s 161us/sample - loss: 2.0434 - acc: 0.2121\n",
      "Epoch 53/500\n",
      "33/33 [==============================] - 0s 192us/sample - loss: 2.0426 - acc: 0.2121\n",
      "Epoch 54/500\n",
      "33/33 [==============================] - 0s 262us/sample - loss: 2.0419 - acc: 0.2121\n",
      "Epoch 55/500\n",
      "33/33 [==============================] - 0s 145us/sample - loss: 2.0412 - acc: 0.2121\n",
      "Epoch 56/500\n",
      "33/33 [==============================] - 0s 86us/sample - loss: 2.0405 - acc: 0.2121\n",
      "Epoch 57/500\n",
      "33/33 [==============================] - 0s 202us/sample - loss: 2.0399 - acc: 0.2121\n",
      "Epoch 58/500\n",
      "33/33 [==============================] - 0s 127us/sample - loss: 2.0390 - acc: 0.2121\n",
      "Epoch 59/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 2.0381 - acc: 0.2121\n",
      "Epoch 60/500\n",
      "33/33 [==============================] - 0s 123us/sample - loss: 2.0370 - acc: 0.2121\n",
      "Epoch 61/500\n",
      "33/33 [==============================] - 0s 257us/sample - loss: 2.0360 - acc: 0.2121\n",
      "Epoch 62/500\n",
      "33/33 [==============================] - 0s 128us/sample - loss: 2.0347 - acc: 0.2121\n",
      "Epoch 63/500\n",
      "33/33 [==============================] - 0s 181us/sample - loss: 2.0336 - acc: 0.2121\n",
      "Epoch 64/500\n",
      "33/33 [==============================] - 0s 238us/sample - loss: 2.0324 - acc: 0.2121\n",
      "Epoch 65/500\n",
      "33/33 [==============================] - 0s 149us/sample - loss: 2.0312 - acc: 0.2121\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 175us/sample - loss: 2.0301 - acc: 0.2121\n",
      "Epoch 67/500\n",
      "33/33 [==============================] - 0s 209us/sample - loss: 2.0291 - acc: 0.2121\n",
      "Epoch 68/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 2.0281 - acc: 0.2121\n",
      "Epoch 69/500\n",
      "33/33 [==============================] - 0s 172us/sample - loss: 2.0274 - acc: 0.2121\n",
      "Epoch 70/500\n",
      "33/33 [==============================] - 0s 245us/sample - loss: 2.0265 - acc: 0.2121\n",
      "Epoch 71/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 2.0254 - acc: 0.2121\n",
      "Epoch 72/500\n",
      "33/33 [==============================] - 0s 305us/sample - loss: 2.0242 - acc: 0.2121\n",
      "Epoch 73/500\n",
      "33/33 [==============================] - 0s 128us/sample - loss: 2.0230 - acc: 0.2121\n",
      "Epoch 74/500\n",
      "33/33 [==============================] - 0s 195us/sample - loss: 2.0219 - acc: 0.2121\n",
      "Epoch 75/500\n",
      "33/33 [==============================] - 0s 135us/sample - loss: 2.0208 - acc: 0.2121\n",
      "Epoch 76/500\n",
      "33/33 [==============================] - 0s 210us/sample - loss: 2.0199 - acc: 0.2121\n",
      "Epoch 77/500\n",
      "33/33 [==============================] - 0s 225us/sample - loss: 2.0189 - acc: 0.2121\n",
      "Epoch 78/500\n",
      "33/33 [==============================] - 0s 163us/sample - loss: 2.0179 - acc: 0.2121\n",
      "Epoch 79/500\n",
      "33/33 [==============================] - 0s 243us/sample - loss: 2.0167 - acc: 0.2121\n",
      "Epoch 80/500\n",
      "33/33 [==============================] - 0s 180us/sample - loss: 2.0156 - acc: 0.2121\n",
      "Epoch 81/500\n",
      "33/33 [==============================] - 0s 485us/sample - loss: 2.0145 - acc: 0.2121\n",
      "Epoch 82/500\n",
      "33/33 [==============================] - 0s 207us/sample - loss: 2.0133 - acc: 0.2121\n",
      "Epoch 83/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 2.0122 - acc: 0.2121\n",
      "Epoch 84/500\n",
      "33/33 [==============================] - 0s 173us/sample - loss: 2.0112 - acc: 0.2121\n",
      "Epoch 85/500\n",
      "33/33 [==============================] - 0s 170us/sample - loss: 2.0101 - acc: 0.2121\n",
      "Epoch 86/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 2.0090 - acc: 0.2121\n",
      "Epoch 87/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 2.0079 - acc: 0.2121\n",
      "Epoch 88/500\n",
      "33/33 [==============================] - 0s 226us/sample - loss: 2.0068 - acc: 0.2121\n",
      "Epoch 89/500\n",
      "33/33 [==============================] - 0s 178us/sample - loss: 2.0057 - acc: 0.2121\n",
      "Epoch 90/500\n",
      "33/33 [==============================] - 0s 149us/sample - loss: 2.0047 - acc: 0.2121\n",
      "Epoch 91/500\n",
      "33/33 [==============================] - 0s 123us/sample - loss: 2.0038 - acc: 0.2121\n",
      "Epoch 92/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 2.0048 - acc: 0.218 - 0s 141us/sample - loss: 2.0029 - acc: 0.2121\n",
      "Epoch 93/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 2.0022 - acc: 0.2121\n",
      "Epoch 94/500\n",
      "33/33 [==============================] - 0s 207us/sample - loss: 2.0012 - acc: 0.2121\n",
      "Epoch 95/500\n",
      "33/33 [==============================] - 0s 215us/sample - loss: 2.0005 - acc: 0.2121\n",
      "Epoch 96/500\n",
      "33/33 [==============================] - 0s 158us/sample - loss: 1.9994 - acc: 0.2121\n",
      "Epoch 97/500\n",
      "33/33 [==============================] - 0s 184us/sample - loss: 1.9979 - acc: 0.2121\n",
      "Epoch 98/500\n",
      "33/33 [==============================] - 0s 193us/sample - loss: 1.9964 - acc: 0.2121\n",
      "Epoch 99/500\n",
      "33/33 [==============================] - 0s 225us/sample - loss: 1.9954 - acc: 0.2121\n",
      "Epoch 100/500\n",
      "33/33 [==============================] - 0s 143us/sample - loss: 1.9942 - acc: 0.2121\n",
      "Epoch 101/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 1.9931 - acc: 0.2121\n",
      "Epoch 102/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.9920 - acc: 0.2121\n",
      "Epoch 103/500\n",
      "33/33 [==============================] - 0s 217us/sample - loss: 1.9906 - acc: 0.2121\n",
      "Epoch 104/500\n",
      "33/33 [==============================] - 0s 173us/sample - loss: 1.9889 - acc: 0.2121\n",
      "Epoch 105/500\n",
      "33/33 [==============================] - 0s 222us/sample - loss: 1.9874 - acc: 0.2121\n",
      "Epoch 106/500\n",
      "33/33 [==============================] - 0s 156us/sample - loss: 1.9853 - acc: 0.2121\n",
      "Epoch 107/500\n",
      "33/33 [==============================] - 0s 197us/sample - loss: 1.9836 - acc: 0.2121\n",
      "Epoch 108/500\n",
      "33/33 [==============================] - 0s 190us/sample - loss: 1.9822 - acc: 0.2121\n",
      "Epoch 109/500\n",
      "33/33 [==============================] - 0s 172us/sample - loss: 1.9806 - acc: 0.2121\n",
      "Epoch 110/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 1.9789 - acc: 0.2121\n",
      "Epoch 111/500\n",
      "33/33 [==============================] - 0s 182us/sample - loss: 1.9772 - acc: 0.2121\n",
      "Epoch 112/500\n",
      "33/33 [==============================] - 0s 228us/sample - loss: 1.9757 - acc: 0.2121\n",
      "Epoch 113/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 1.9740 - acc: 0.2121\n",
      "Epoch 114/500\n",
      "33/33 [==============================] - 0s 154us/sample - loss: 1.9724 - acc: 0.2121\n",
      "Epoch 115/500\n",
      "33/33 [==============================] - 0s 276us/sample - loss: 1.9708 - acc: 0.2121\n",
      "Epoch 116/500\n",
      "33/33 [==============================] - 0s 182us/sample - loss: 1.9688 - acc: 0.2121\n",
      "Epoch 117/500\n",
      "33/33 [==============================] - 0s 157us/sample - loss: 1.9669 - acc: 0.2121\n",
      "Epoch 118/500\n",
      "33/33 [==============================] - 0s 197us/sample - loss: 1.9651 - acc: 0.2121\n",
      "Epoch 119/500\n",
      "33/33 [==============================] - 0s 139us/sample - loss: 1.9629 - acc: 0.2121\n",
      "Epoch 120/500\n",
      "33/33 [==============================] - 0s 233us/sample - loss: 1.9612 - acc: 0.2121\n",
      "Epoch 121/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.9597 - acc: 0.2121\n",
      "Epoch 122/500\n",
      "33/33 [==============================] - 0s 119us/sample - loss: 1.9580 - acc: 0.2121\n",
      "Epoch 123/500\n",
      "33/33 [==============================] - 0s 189us/sample - loss: 1.9567 - acc: 0.2121\n",
      "Epoch 124/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 1.9552 - acc: 0.2121\n",
      "Epoch 125/500\n",
      "33/33 [==============================] - 0s 229us/sample - loss: 1.9532 - acc: 0.2121\n",
      "Epoch 126/500\n",
      "33/33 [==============================] - 0s 158us/sample - loss: 1.9515 - acc: 0.2121\n",
      "Epoch 127/500\n",
      "33/33 [==============================] - 0s 155us/sample - loss: 1.9497 - acc: 0.2121\n",
      "Epoch 128/500\n",
      "33/33 [==============================] - 0s 125us/sample - loss: 1.9478 - acc: 0.2121\n",
      "Epoch 129/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 1.9467 - acc: 0.2121\n",
      "Epoch 130/500\n",
      "33/33 [==============================] - 0s 253us/sample - loss: 1.9444 - acc: 0.2121\n",
      "Epoch 131/500\n",
      "33/33 [==============================] - 0s 109us/sample - loss: 1.9425 - acc: 0.2121\n",
      "Epoch 132/500\n",
      "33/33 [==============================] - 0s 179us/sample - loss: 1.9399 - acc: 0.2121\n",
      "Epoch 133/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 1.9374 - acc: 0.2121\n",
      "Epoch 134/500\n",
      "33/33 [==============================] - 0s 238us/sample - loss: 1.9349 - acc: 0.2121\n",
      "Epoch 135/500\n",
      "33/33 [==============================] - 0s 148us/sample - loss: 1.9325 - acc: 0.2121\n",
      "Epoch 136/500\n",
      "33/33 [==============================] - 0s 204us/sample - loss: 1.9298 - acc: 0.2121\n",
      "Epoch 137/500\n",
      "33/33 [==============================] - 0s 132us/sample - loss: 1.9273 - acc: 0.2121\n",
      "Epoch 138/500\n",
      "33/33 [==============================] - 0s 157us/sample - loss: 1.9247 - acc: 0.2121\n",
      "Epoch 139/500\n",
      "33/33 [==============================] - 0s 275us/sample - loss: 1.9223 - acc: 0.2121\n",
      "Epoch 140/500\n",
      "33/33 [==============================] - 0s 153us/sample - loss: 1.9198 - acc: 0.2121\n",
      "Epoch 141/500\n",
      "33/33 [==============================] - 0s 162us/sample - loss: 1.9176 - acc: 0.2121\n",
      "Epoch 142/500\n",
      "33/33 [==============================] - 0s 135us/sample - loss: 1.9147 - acc: 0.2121\n",
      "Epoch 143/500\n",
      "33/33 [==============================] - 0s 193us/sample - loss: 1.9117 - acc: 0.2121\n",
      "Epoch 144/500\n",
      "33/33 [==============================] - 0s 163us/sample - loss: 1.9089 - acc: 0.2121\n",
      "Epoch 145/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 1.9059 - acc: 0.2121\n",
      "Epoch 146/500\n",
      "33/33 [==============================] - 0s 148us/sample - loss: 1.9029 - acc: 0.2121\n",
      "Epoch 147/500\n",
      "33/33 [==============================] - 0s 227us/sample - loss: 1.9000 - acc: 0.2121\n",
      "Epoch 148/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 139us/sample - loss: 1.8977 - acc: 0.2121\n",
      "Epoch 149/500\n",
      "33/33 [==============================] - 0s 166us/sample - loss: 1.8949 - acc: 0.2121\n",
      "Epoch 150/500\n",
      "33/33 [==============================] - 0s 105us/sample - loss: 1.8929 - acc: 0.2121\n",
      "Epoch 151/500\n",
      "33/33 [==============================] - 0s 216us/sample - loss: 1.8921 - acc: 0.2121\n",
      "Epoch 152/500\n",
      "33/33 [==============================] - 0s 179us/sample - loss: 1.8905 - acc: 0.2121\n",
      "Epoch 153/500\n",
      "33/33 [==============================] - 0s 149us/sample - loss: 1.8880 - acc: 0.2121\n",
      "Epoch 154/500\n",
      "33/33 [==============================] - 0s 181us/sample - loss: 1.8849 - acc: 0.2121\n",
      "Epoch 155/500\n",
      "33/33 [==============================] - 0s 122us/sample - loss: 1.8819 - acc: 0.2121\n",
      "Epoch 156/500\n",
      "33/33 [==============================] - 0s 227us/sample - loss: 1.8794 - acc: 0.2121\n",
      "Epoch 157/500\n",
      "33/33 [==============================] - 0s 102us/sample - loss: 1.8769 - acc: 0.2121\n",
      "Epoch 158/500\n",
      "33/33 [==============================] - 0s 221us/sample - loss: 1.8746 - acc: 0.2121\n",
      "Epoch 159/500\n",
      "33/33 [==============================] - 0s 140us/sample - loss: 1.8725 - acc: 0.2121\n",
      "Epoch 160/500\n",
      "33/33 [==============================] - 0s 175us/sample - loss: 1.8699 - acc: 0.2121\n",
      "Epoch 161/500\n",
      "33/33 [==============================] - 0s 184us/sample - loss: 1.8672 - acc: 0.2121\n",
      "Epoch 162/500\n",
      "33/33 [==============================] - 0s 136us/sample - loss: 1.8638 - acc: 0.2121\n",
      "Epoch 163/500\n",
      "33/33 [==============================] - 0s 216us/sample - loss: 1.8599 - acc: 0.2121\n",
      "Epoch 164/500\n",
      "33/33 [==============================] - 0s 184us/sample - loss: 1.8560 - acc: 0.2121\n",
      "Epoch 165/500\n",
      "33/33 [==============================] - 0s 207us/sample - loss: 1.8527 - acc: 0.2121\n",
      "Epoch 166/500\n",
      "33/33 [==============================] - 0s 152us/sample - loss: 1.8496 - acc: 0.2121\n",
      "Epoch 167/500\n",
      "33/33 [==============================] - 0s 128us/sample - loss: 1.8466 - acc: 0.2121\n",
      "Epoch 168/500\n",
      "33/33 [==============================] - 0s 116us/sample - loss: 1.8433 - acc: 0.2121\n",
      "Epoch 169/500\n",
      "33/33 [==============================] - 0s 274us/sample - loss: 1.8402 - acc: 0.2121\n",
      "Epoch 170/500\n",
      "33/33 [==============================] - 0s 184us/sample - loss: 1.8373 - acc: 0.2121\n",
      "Epoch 171/500\n",
      "33/33 [==============================] - 0s 228us/sample - loss: 1.8343 - acc: 0.2121\n",
      "Epoch 172/500\n",
      "33/33 [==============================] - 0s 185us/sample - loss: 1.8315 - acc: 0.2121\n",
      "Epoch 173/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.8283 - acc: 0.2121\n",
      "Epoch 174/500\n",
      "33/33 [==============================] - 0s 137us/sample - loss: 1.8243 - acc: 0.2121\n",
      "Epoch 175/500\n",
      "33/33 [==============================] - 0s 132us/sample - loss: 1.8207 - acc: 0.2121\n",
      "Epoch 176/500\n",
      "33/33 [==============================] - 0s 116us/sample - loss: 1.8171 - acc: 0.2121\n",
      "Epoch 177/500\n",
      "33/33 [==============================] - 0s 143us/sample - loss: 1.8131 - acc: 0.2121\n",
      "Epoch 178/500\n",
      "33/33 [==============================] - 0s 105us/sample - loss: 1.8084 - acc: 0.2121\n",
      "Epoch 179/500\n",
      "33/33 [==============================] - 0s 161us/sample - loss: 1.8043 - acc: 0.2121\n",
      "Epoch 180/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 1.7995 - acc: 0.2121\n",
      "Epoch 181/500\n",
      "33/33 [==============================] - 0s 210us/sample - loss: 1.7952 - acc: 0.2121\n",
      "Epoch 182/500\n",
      "33/33 [==============================] - 0s 108us/sample - loss: 1.7927 - acc: 0.2121\n",
      "Epoch 183/500\n",
      "33/33 [==============================] - 0s 173us/sample - loss: 1.7903 - acc: 0.2121\n",
      "Epoch 184/500\n",
      "33/33 [==============================] - 0s 204us/sample - loss: 1.7874 - acc: 0.2121\n",
      "Epoch 185/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 1.7854 - acc: 0.2121\n",
      "Epoch 186/500\n",
      "33/33 [==============================] - 0s 173us/sample - loss: 1.7822 - acc: 0.2121\n",
      "Epoch 187/500\n",
      "33/33 [==============================] - 0s 203us/sample - loss: 1.7775 - acc: 0.2121\n",
      "Epoch 188/500\n",
      "33/33 [==============================] - 0s 133us/sample - loss: 1.7726 - acc: 0.2121\n",
      "Epoch 189/500\n",
      "33/33 [==============================] - 0s 204us/sample - loss: 1.7677 - acc: 0.2121\n",
      "Epoch 190/500\n",
      "33/33 [==============================] - 0s 134us/sample - loss: 1.7630 - acc: 0.2121\n",
      "Epoch 191/500\n",
      "33/33 [==============================] - 0s 247us/sample - loss: 1.7588 - acc: 0.2121\n",
      "Epoch 192/500\n",
      "33/33 [==============================] - 0s 300us/sample - loss: 1.7547 - acc: 0.2121\n",
      "Epoch 193/500\n",
      "33/33 [==============================] - 0s 146us/sample - loss: 1.7507 - acc: 0.2121\n",
      "Epoch 194/500\n",
      "33/33 [==============================] - 0s 273us/sample - loss: 1.7478 - acc: 0.2121\n",
      "Epoch 195/500\n",
      "33/33 [==============================] - 0s 253us/sample - loss: 1.7447 - acc: 0.2121\n",
      "Epoch 196/500\n",
      "33/33 [==============================] - 0s 189us/sample - loss: 1.7396 - acc: 0.2121\n",
      "Epoch 197/500\n",
      "33/33 [==============================] - 0s 189us/sample - loss: 1.7339 - acc: 0.2121\n",
      "Epoch 198/500\n",
      "33/33 [==============================] - 0s 196us/sample - loss: 1.7282 - acc: 0.2121\n",
      "Epoch 199/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.7237 - acc: 0.2121\n",
      "Epoch 200/500\n",
      "33/33 [==============================] - 0s 199us/sample - loss: 1.7197 - acc: 0.2121\n",
      "Epoch 201/500\n",
      "33/33 [==============================] - 0s 180us/sample - loss: 1.7169 - acc: 0.2121\n",
      "Epoch 202/500\n",
      "33/33 [==============================] - 0s 219us/sample - loss: 1.7145 - acc: 0.2121\n",
      "Epoch 203/500\n",
      "33/33 [==============================] - 0s 149us/sample - loss: 1.7118 - acc: 0.2121\n",
      "Epoch 204/500\n",
      "33/33 [==============================] - 0s 188us/sample - loss: 1.7092 - acc: 0.2121\n",
      "Epoch 205/500\n",
      "33/33 [==============================] - 0s 114us/sample - loss: 1.7073 - acc: 0.2121\n",
      "Epoch 206/500\n",
      "33/33 [==============================] - 0s 239us/sample - loss: 1.7041 - acc: 0.2121\n",
      "Epoch 207/500\n",
      "33/33 [==============================] - 0s 296us/sample - loss: 1.7001 - acc: 0.2121\n",
      "Epoch 208/500\n",
      "33/33 [==============================] - 0s 178us/sample - loss: 1.6961 - acc: 0.2121\n",
      "Epoch 209/500\n",
      "33/33 [==============================] - 0s 232us/sample - loss: 1.6922 - acc: 0.2121\n",
      "Epoch 210/500\n",
      "33/33 [==============================] - 0s 112us/sample - loss: 1.6884 - acc: 0.2121\n",
      "Epoch 211/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 1.6836 - acc: 0.2121\n",
      "Epoch 212/500\n",
      "33/33 [==============================] - 0s 126us/sample - loss: 1.6782 - acc: 0.2121\n",
      "Epoch 213/500\n",
      "33/33 [==============================] - 0s 111us/sample - loss: 1.6731 - acc: 0.2121\n",
      "Epoch 214/500\n",
      "33/33 [==============================] - 0s 227us/sample - loss: 1.6686 - acc: 0.2121\n",
      "Epoch 215/500\n",
      "33/33 [==============================] - 0s 177us/sample - loss: 1.6638 - acc: 0.2121\n",
      "Epoch 216/500\n",
      "33/33 [==============================] - 0s 261us/sample - loss: 1.6596 - acc: 0.2121\n",
      "Epoch 217/500\n",
      "33/33 [==============================] - 0s 159us/sample - loss: 1.6549 - acc: 0.2121\n",
      "Epoch 218/500\n",
      "33/33 [==============================] - 0s 170us/sample - loss: 1.6509 - acc: 0.2121\n",
      "Epoch 219/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 1.6470 - acc: 0.2121\n",
      "Epoch 220/500\n",
      "33/33 [==============================] - 0s 145us/sample - loss: 1.6430 - acc: 0.2121\n",
      "Epoch 221/500\n",
      "33/33 [==============================] - 0s 320us/sample - loss: 1.6389 - acc: 0.2121\n",
      "Epoch 222/500\n",
      "33/33 [==============================] - 0s 121us/sample - loss: 1.6345 - acc: 0.2121\n",
      "Epoch 223/500\n",
      "33/33 [==============================] - 0s 149us/sample - loss: 1.6302 - acc: 0.2121\n",
      "Epoch 224/500\n",
      "33/33 [==============================] - 0s 230us/sample - loss: 1.6260 - acc: 0.2121\n",
      "Epoch 225/500\n",
      "33/33 [==============================] - 0s 135us/sample - loss: 1.6220 - acc: 0.2121\n",
      "Epoch 226/500\n",
      "33/33 [==============================] - 0s 264us/sample - loss: 1.6184 - acc: 0.2121\n",
      "Epoch 227/500\n",
      "33/33 [==============================] - 0s 154us/sample - loss: 1.6148 - acc: 0.2121\n",
      "Epoch 228/500\n",
      "33/33 [==============================] - 0s 221us/sample - loss: 1.6111 - acc: 0.2727\n",
      "Epoch 229/500\n",
      "33/33 [==============================] - 0s 139us/sample - loss: 1.6079 - acc: 0.3030\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 100us/sample - loss: 1.6048 - acc: 0.3333\n",
      "Epoch 231/500\n",
      "33/33 [==============================] - 0s 121us/sample - loss: 1.6009 - acc: 0.3333\n",
      "Epoch 232/500\n",
      "33/33 [==============================] - 0s 143us/sample - loss: 1.5966 - acc: 0.3333\n",
      "Epoch 233/500\n",
      "33/33 [==============================] - 0s 286us/sample - loss: 1.5920 - acc: 0.3333\n",
      "Epoch 234/500\n",
      "33/33 [==============================] - 0s 148us/sample - loss: 1.5867 - acc: 0.3333\n",
      "Epoch 235/500\n",
      "33/33 [==============================] - 0s 147us/sample - loss: 1.5814 - acc: 0.3030\n",
      "Epoch 236/500\n",
      "33/33 [==============================] - 0s 136us/sample - loss: 1.5765 - acc: 0.3030\n",
      "Epoch 237/500\n",
      "33/33 [==============================] - 0s 153us/sample - loss: 1.5714 - acc: 0.3030\n",
      "Epoch 238/500\n",
      "33/33 [==============================] - 0s 162us/sample - loss: 1.5668 - acc: 0.3030\n",
      "Epoch 239/500\n",
      "33/33 [==============================] - 0s 133us/sample - loss: 1.5621 - acc: 0.3030\n",
      "Epoch 240/500\n",
      "33/33 [==============================] - 0s 291us/sample - loss: 1.5574 - acc: 0.3030\n",
      "Epoch 241/500\n",
      "33/33 [==============================] - 0s 101us/sample - loss: 1.5531 - acc: 0.3030\n",
      "Epoch 242/500\n",
      "33/33 [==============================] - 0s 110us/sample - loss: 1.5491 - acc: 0.3030\n",
      "Epoch 243/500\n",
      "33/33 [==============================] - 0s 134us/sample - loss: 1.5449 - acc: 0.3030\n",
      "Epoch 244/500\n",
      "33/33 [==============================] - 0s 240us/sample - loss: 1.5410 - acc: 0.3030\n",
      "Epoch 245/500\n",
      "33/33 [==============================] - 0s 222us/sample - loss: 1.5376 - acc: 0.3333\n",
      "Epoch 246/500\n",
      "33/33 [==============================] - 0s 191us/sample - loss: 1.5333 - acc: 0.3333\n",
      "Epoch 247/500\n",
      "33/33 [==============================] - 0s 153us/sample - loss: 1.5281 - acc: 0.3333\n",
      "Epoch 248/500\n",
      "33/33 [==============================] - 0s 135us/sample - loss: 1.5229 - acc: 0.3333\n",
      "Epoch 249/500\n",
      "33/33 [==============================] - 0s 175us/sample - loss: 1.5180 - acc: 0.3333\n",
      "Epoch 250/500\n",
      "33/33 [==============================] - 0s 113us/sample - loss: 1.5134 - acc: 0.3333\n",
      "Epoch 251/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 1.5086 - acc: 0.3333\n",
      "Epoch 252/500\n",
      "33/33 [==============================] - 0s 203us/sample - loss: 1.5037 - acc: 0.3333\n",
      "Epoch 253/500\n",
      "33/33 [==============================] - 0s 147us/sample - loss: 1.4987 - acc: 0.3333\n",
      "Epoch 254/500\n",
      "33/33 [==============================] - 0s 193us/sample - loss: 1.4940 - acc: 0.3030\n",
      "Epoch 255/500\n",
      "33/33 [==============================] - 0s 109us/sample - loss: 1.4895 - acc: 0.3333\n",
      "Epoch 256/500\n",
      "33/33 [==============================] - 0s 140us/sample - loss: 1.4851 - acc: 0.3333\n",
      "Epoch 257/500\n",
      "33/33 [==============================] - 0s 155us/sample - loss: 1.4804 - acc: 0.3030\n",
      "Epoch 258/500\n",
      "33/33 [==============================] - 0s 147us/sample - loss: 1.4757 - acc: 0.3939\n",
      "Epoch 259/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 1.4710 - acc: 0.4242\n",
      "Epoch 260/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 1.4662 - acc: 0.4242\n",
      "Epoch 261/500\n",
      "33/33 [==============================] - 0s 203us/sample - loss: 1.4616 - acc: 0.4242\n",
      "Epoch 262/500\n",
      "33/33 [==============================] - 0s 142us/sample - loss: 1.4571 - acc: 0.4242\n",
      "Epoch 263/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.4524 - acc: 0.4242\n",
      "Epoch 264/500\n",
      "33/33 [==============================] - 0s 152us/sample - loss: 1.4478 - acc: 0.4242\n",
      "Epoch 265/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 1.4434 - acc: 0.4242\n",
      "Epoch 266/500\n",
      "33/33 [==============================] - 0s 166us/sample - loss: 1.4397 - acc: 0.4242\n",
      "Epoch 267/500\n",
      "33/33 [==============================] - 0s 143us/sample - loss: 1.4360 - acc: 0.4242\n",
      "Epoch 268/500\n",
      "33/33 [==============================] - 0s 131us/sample - loss: 1.4321 - acc: 0.4545\n",
      "Epoch 269/500\n",
      "33/33 [==============================] - 0s 128us/sample - loss: 1.4292 - acc: 0.4848\n",
      "Epoch 270/500\n",
      "33/33 [==============================] - 0s 154us/sample - loss: 1.4259 - acc: 0.4848\n",
      "Epoch 271/500\n",
      "33/33 [==============================] - 0s 142us/sample - loss: 1.4221 - acc: 0.4848\n",
      "Epoch 272/500\n",
      "33/33 [==============================] - 0s 193us/sample - loss: 1.4177 - acc: 0.4848\n",
      "Epoch 273/500\n",
      "33/33 [==============================] - 0s 121us/sample - loss: 1.4134 - acc: 0.4848\n",
      "Epoch 274/500\n",
      "33/33 [==============================] - 0s 251us/sample - loss: 1.4089 - acc: 0.4848\n",
      "Epoch 275/500\n",
      "33/33 [==============================] - 0s 290us/sample - loss: 1.4042 - acc: 0.4848\n",
      "Epoch 276/500\n",
      "33/33 [==============================] - 0s 117us/sample - loss: 1.3998 - acc: 0.4848\n",
      "Epoch 277/500\n",
      "33/33 [==============================] - 0s 219us/sample - loss: 1.3960 - acc: 0.4848\n",
      "Epoch 278/500\n",
      "33/33 [==============================] - 0s 168us/sample - loss: 1.3919 - acc: 0.4848\n",
      "Epoch 279/500\n",
      "33/33 [==============================] - 0s 202us/sample - loss: 1.3878 - acc: 0.4848\n",
      "Epoch 280/500\n",
      "33/33 [==============================] - 0s 188us/sample - loss: 1.3838 - acc: 0.4848\n",
      "Epoch 281/500\n",
      "33/33 [==============================] - 0s 127us/sample - loss: 1.3800 - acc: 0.5455\n",
      "Epoch 282/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 1.3758 - acc: 0.5758\n",
      "Epoch 283/500\n",
      "33/33 [==============================] - 0s 115us/sample - loss: 1.3721 - acc: 0.5758\n",
      "Epoch 284/500\n",
      "33/33 [==============================] - 0s 228us/sample - loss: 1.3694 - acc: 0.5758\n",
      "Epoch 285/500\n",
      "33/33 [==============================] - 0s 180us/sample - loss: 1.3678 - acc: 0.6061\n",
      "Epoch 286/500\n",
      "33/33 [==============================] - 0s 147us/sample - loss: 1.3663 - acc: 0.6061\n",
      "Epoch 287/500\n",
      "33/33 [==============================] - 0s 201us/sample - loss: 1.3644 - acc: 0.6061\n",
      "Epoch 288/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 1.3622 - acc: 0.6061\n",
      "Epoch 289/500\n",
      "33/33 [==============================] - 0s 148us/sample - loss: 1.3596 - acc: 0.6061\n",
      "Epoch 290/500\n",
      "33/33 [==============================] - 0s 270us/sample - loss: 1.3550 - acc: 0.6061\n",
      "Epoch 291/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 1.3491 - acc: 0.6061\n",
      "Epoch 292/500\n",
      "33/33 [==============================] - 0s 264us/sample - loss: 1.3428 - acc: 0.6061\n",
      "Epoch 293/500\n",
      "33/33 [==============================] - 0s 343us/sample - loss: 1.3370 - acc: 0.6061\n",
      "Epoch 294/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 1.3319 - acc: 0.6061\n",
      "Epoch 295/500\n",
      "33/33 [==============================] - 0s 192us/sample - loss: 1.3281 - acc: 0.6061\n",
      "Epoch 296/500\n",
      "33/33 [==============================] - 0s 179us/sample - loss: 1.3245 - acc: 0.6061\n",
      "Epoch 297/500\n",
      "33/33 [==============================] - 0s 186us/sample - loss: 1.3216 - acc: 0.6061\n",
      "Epoch 298/500\n",
      "33/33 [==============================] - 0s 153us/sample - loss: 1.3199 - acc: 0.5455\n",
      "Epoch 299/500\n",
      "33/33 [==============================] - 0s 174us/sample - loss: 1.3207 - acc: 0.4848\n",
      "Epoch 300/500\n",
      "33/33 [==============================] - 0s 178us/sample - loss: 1.3206 - acc: 0.4545\n",
      "Epoch 301/500\n",
      "33/33 [==============================] - 0s 130us/sample - loss: 1.3187 - acc: 0.4545\n",
      "Epoch 302/500\n",
      "33/33 [==============================] - 0s 116us/sample - loss: 1.3148 - acc: 0.4545\n",
      "Epoch 303/500\n",
      "33/33 [==============================] - 0s 118us/sample - loss: 1.3087 - acc: 0.4848\n",
      "Epoch 304/500\n",
      "33/33 [==============================] - 0s 223us/sample - loss: 1.3031 - acc: 0.4848\n",
      "Epoch 305/500\n",
      "33/33 [==============================] - 0s 118us/sample - loss: 1.2984 - acc: 0.4848\n",
      "Epoch 306/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 1.2933 - acc: 0.5455\n",
      "Epoch 307/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 1.2881 - acc: 0.6061\n",
      "Epoch 308/500\n",
      "33/33 [==============================] - 0s 256us/sample - loss: 1.2836 - acc: 0.6061\n",
      "Epoch 309/500\n",
      "33/33 [==============================] - 0s 256us/sample - loss: 1.2798 - acc: 0.6061\n",
      "Epoch 310/500\n",
      "33/33 [==============================] - 0s 156us/sample - loss: 1.2767 - acc: 0.6061\n",
      "Epoch 311/500\n",
      "33/33 [==============================] - 0s 266us/sample - loss: 1.2744 - acc: 0.6061\n",
      "Epoch 312/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 153us/sample - loss: 1.2718 - acc: 0.6061\n",
      "Epoch 313/500\n",
      "33/33 [==============================] - 0s 158us/sample - loss: 1.2694 - acc: 0.6061\n",
      "Epoch 314/500\n",
      "33/33 [==============================] - 0s 277us/sample - loss: 1.2661 - acc: 0.6061\n",
      "Epoch 315/500\n",
      "33/33 [==============================] - 0s 123us/sample - loss: 1.2628 - acc: 0.6061\n",
      "Epoch 316/500\n",
      "33/33 [==============================] - 0s 263us/sample - loss: 1.2590 - acc: 0.6061\n",
      "Epoch 317/500\n",
      "33/33 [==============================] - 0s 172us/sample - loss: 1.2556 - acc: 0.6061\n",
      "Epoch 318/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.2517 - acc: 0.6061\n",
      "Epoch 319/500\n",
      "33/33 [==============================] - 0s 127us/sample - loss: 1.2485 - acc: 0.6061\n",
      "Epoch 320/500\n",
      "33/33 [==============================] - 0s 255us/sample - loss: 1.2449 - acc: 0.6061\n",
      "Epoch 321/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 1.2415 - acc: 0.6061\n",
      "Epoch 322/500\n",
      "33/33 [==============================] - 0s 123us/sample - loss: 1.2383 - acc: 0.6061\n",
      "Epoch 323/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 1.2357 - acc: 0.6061\n",
      "Epoch 324/500\n",
      "33/33 [==============================] - 0s 102us/sample - loss: 1.2336 - acc: 0.6061\n",
      "Epoch 325/500\n",
      "33/33 [==============================] - 0s 202us/sample - loss: 1.2315 - acc: 0.6061\n",
      "Epoch 326/500\n",
      "33/33 [==============================] - 0s 273us/sample - loss: 1.2299 - acc: 0.6061\n",
      "Epoch 327/500\n",
      "33/33 [==============================] - 0s 117us/sample - loss: 1.2286 - acc: 0.5758\n",
      "Epoch 328/500\n",
      "33/33 [==============================] - 0s 189us/sample - loss: 1.2272 - acc: 0.5758\n",
      "Epoch 329/500\n",
      "33/33 [==============================] - 0s 121us/sample - loss: 1.2242 - acc: 0.5758\n",
      "Epoch 330/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 1.2206 - acc: 0.6061\n",
      "Epoch 331/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 1.2170 - acc: 0.6061\n",
      "Epoch 332/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 1.2136 - acc: 0.6061\n",
      "Epoch 333/500\n",
      "33/33 [==============================] - 0s 134us/sample - loss: 1.2104 - acc: 0.6061\n",
      "Epoch 334/500\n",
      "33/33 [==============================] - 0s 126us/sample - loss: 1.2066 - acc: 0.6061\n",
      "Epoch 335/500\n",
      "33/33 [==============================] - 0s 139us/sample - loss: 1.2033 - acc: 0.6061\n",
      "Epoch 336/500\n",
      "33/33 [==============================] - 0s 161us/sample - loss: 1.2008 - acc: 0.6061\n",
      "Epoch 337/500\n",
      "33/33 [==============================] - 0s 218us/sample - loss: 1.1990 - acc: 0.6061\n",
      "Epoch 338/500\n",
      "33/33 [==============================] - 0s 189us/sample - loss: 1.1969 - acc: 0.6061\n",
      "Epoch 339/500\n",
      "33/33 [==============================] - 0s 241us/sample - loss: 1.1937 - acc: 0.6061\n",
      "Epoch 340/500\n",
      "33/33 [==============================] - 0s 145us/sample - loss: 1.1899 - acc: 0.6061\n",
      "Epoch 341/500\n",
      "33/33 [==============================] - 0s 194us/sample - loss: 1.1861 - acc: 0.6061\n",
      "Epoch 342/500\n",
      "33/33 [==============================] - 0s 135us/sample - loss: 1.1819 - acc: 0.6061\n",
      "Epoch 343/500\n",
      "33/33 [==============================] - 0s 144us/sample - loss: 1.1780 - acc: 0.6061\n",
      "Epoch 344/500\n",
      "33/33 [==============================] - 0s 179us/sample - loss: 1.1740 - acc: 0.6061\n",
      "Epoch 345/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 1.1712 - acc: 0.6061\n",
      "Epoch 346/500\n",
      "33/33 [==============================] - 0s 248us/sample - loss: 1.1685 - acc: 0.6061\n",
      "Epoch 347/500\n",
      "33/33 [==============================] - 0s 190us/sample - loss: 1.1666 - acc: 0.6061\n",
      "Epoch 348/500\n",
      "33/33 [==============================] - 0s 170us/sample - loss: 1.1653 - acc: 0.6061\n",
      "Epoch 349/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 1.1646 - acc: 0.6061\n",
      "Epoch 350/500\n",
      "33/33 [==============================] - 0s 127us/sample - loss: 1.1634 - acc: 0.6061\n",
      "Epoch 351/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 1.1609 - acc: 0.6061\n",
      "Epoch 352/500\n",
      "33/33 [==============================] - 0s 155us/sample - loss: 1.1587 - acc: 0.6061\n",
      "Epoch 353/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.1559 - acc: 0.6061\n",
      "Epoch 354/500\n",
      "33/33 [==============================] - 0s 199us/sample - loss: 1.1536 - acc: 0.6061\n",
      "Epoch 355/500\n",
      "33/33 [==============================] - 0s 137us/sample - loss: 1.1512 - acc: 0.6061\n",
      "Epoch 356/500\n",
      "33/33 [==============================] - 0s 225us/sample - loss: 1.1491 - acc: 0.6364\n",
      "Epoch 357/500\n",
      "33/33 [==============================] - 0s 126us/sample - loss: 1.1468 - acc: 0.6667\n",
      "Epoch 358/500\n",
      "33/33 [==============================] - 0s 192us/sample - loss: 1.1436 - acc: 0.6667\n",
      "Epoch 359/500\n",
      "33/33 [==============================] - 0s 140us/sample - loss: 1.1402 - acc: 0.6667\n",
      "Epoch 360/500\n",
      "33/33 [==============================] - 0s 219us/sample - loss: 1.1360 - acc: 0.6667\n",
      "Epoch 361/500\n",
      "33/33 [==============================] - 0s 196us/sample - loss: 1.1315 - acc: 0.6667\n",
      "Epoch 362/500\n",
      "33/33 [==============================] - 0s 210us/sample - loss: 1.1277 - acc: 0.6667\n",
      "Epoch 363/500\n",
      "33/33 [==============================] - 0s 195us/sample - loss: 1.1242 - acc: 0.6970\n",
      "Epoch 364/500\n",
      "33/33 [==============================] - 0s 158us/sample - loss: 1.1210 - acc: 0.6970\n",
      "Epoch 365/500\n",
      "33/33 [==============================] - 0s 229us/sample - loss: 1.1183 - acc: 0.6364\n",
      "Epoch 366/500\n",
      "33/33 [==============================] - 0s 134us/sample - loss: 1.1165 - acc: 0.6364\n",
      "Epoch 367/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 1.1147 - acc: 0.6061\n",
      "Epoch 368/500\n",
      "33/33 [==============================] - 0s 142us/sample - loss: 1.1132 - acc: 0.6061\n",
      "Epoch 369/500\n",
      "33/33 [==============================] - 0s 254us/sample - loss: 1.1106 - acc: 0.6061\n",
      "Epoch 370/500\n",
      "33/33 [==============================] - 0s 128us/sample - loss: 1.1074 - acc: 0.6364\n",
      "Epoch 371/500\n",
      "33/33 [==============================] - 0s 125us/sample - loss: 1.1039 - acc: 0.6667\n",
      "Epoch 372/500\n",
      "33/33 [==============================] - 0s 161us/sample - loss: 1.1010 - acc: 0.6667\n",
      "Epoch 373/500\n",
      "33/33 [==============================] - 0s 127us/sample - loss: 1.0984 - acc: 0.6667\n",
      "Epoch 374/500\n",
      "33/33 [==============================] - 0s 204us/sample - loss: 1.0969 - acc: 0.6061\n",
      "Epoch 375/500\n",
      "33/33 [==============================] - 0s 141us/sample - loss: 1.0964 - acc: 0.5455\n",
      "Epoch 376/500\n",
      "33/33 [==============================] - 0s 245us/sample - loss: 1.0960 - acc: 0.5455\n",
      "Epoch 377/500\n",
      "33/33 [==============================] - 0s 294us/sample - loss: 1.0941 - acc: 0.5455\n",
      "Epoch 378/500\n",
      "33/33 [==============================] - 0s 154us/sample - loss: 1.0914 - acc: 0.5455\n",
      "Epoch 379/500\n",
      "33/33 [==============================] - 0s 313us/sample - loss: 1.0891 - acc: 0.5455\n",
      "Epoch 380/500\n",
      "33/33 [==============================] - 0s 302us/sample - loss: 1.0861 - acc: 0.5758\n",
      "Epoch 381/500\n",
      "33/33 [==============================] - 0s 144us/sample - loss: 1.0820 - acc: 0.5758\n",
      "Epoch 382/500\n",
      "33/33 [==============================] - 0s 200us/sample - loss: 1.0773 - acc: 0.5758\n",
      "Epoch 383/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 1.0729 - acc: 0.6970\n",
      "Epoch 384/500\n",
      "33/33 [==============================] - 0s 144us/sample - loss: 1.0677 - acc: 0.7273\n",
      "Epoch 385/500\n",
      "33/33 [==============================] - 0s 139us/sample - loss: 1.0638 - acc: 0.6667\n",
      "Epoch 386/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 1.0603 - acc: 0.6667\n",
      "Epoch 387/500\n",
      "33/33 [==============================] - 0s 261us/sample - loss: 1.0577 - acc: 0.6364\n",
      "Epoch 388/500\n",
      "33/33 [==============================] - 0s 149us/sample - loss: 1.0552 - acc: 0.6364\n",
      "Epoch 389/500\n",
      "33/33 [==============================] - 0s 147us/sample - loss: 1.0522 - acc: 0.6061\n",
      "Epoch 390/500\n",
      "33/33 [==============================] - 0s 125us/sample - loss: 1.0494 - acc: 0.6061\n",
      "Epoch 391/500\n",
      "33/33 [==============================] - 0s 110us/sample - loss: 1.0459 - acc: 0.6364\n",
      "Epoch 392/500\n",
      "33/33 [==============================] - 0s 160us/sample - loss: 1.0434 - acc: 0.6667\n",
      "Epoch 393/500\n",
      "33/33 [==============================] - 0s 112us/sample - loss: 1.0405 - acc: 0.6970\n",
      "Epoch 394/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 124us/sample - loss: 1.0379 - acc: 0.7273\n",
      "Epoch 395/500\n",
      "33/33 [==============================] - 0s 229us/sample - loss: 1.0358 - acc: 0.7273\n",
      "Epoch 396/500\n",
      "33/33 [==============================] - 0s 153us/sample - loss: 1.0333 - acc: 0.7273\n",
      "Epoch 397/500\n",
      "33/33 [==============================] - 0s 137us/sample - loss: 1.0307 - acc: 0.7273\n",
      "Epoch 398/500\n",
      "33/33 [==============================] - 0s 190us/sample - loss: 1.0279 - acc: 0.7273\n",
      "Epoch 399/500\n",
      "33/33 [==============================] - 0s 136us/sample - loss: 1.0250 - acc: 0.7273\n",
      "Epoch 400/500\n",
      "33/33 [==============================] - 0s 221us/sample - loss: 1.0224 - acc: 0.7576\n",
      "Epoch 401/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 1.0200 - acc: 0.7273\n",
      "Epoch 402/500\n",
      "33/33 [==============================] - 0s 226us/sample - loss: 1.0175 - acc: 0.7879\n",
      "Epoch 403/500\n",
      "33/33 [==============================] - 0s 123us/sample - loss: 1.0154 - acc: 0.7879\n",
      "Epoch 404/500\n",
      "33/33 [==============================] - 0s 231us/sample - loss: 1.0132 - acc: 0.7879\n",
      "Epoch 405/500\n",
      "33/33 [==============================] - 0s 211us/sample - loss: 1.0112 - acc: 0.7879\n",
      "Epoch 406/500\n",
      "33/33 [==============================] - 0s 226us/sample - loss: 1.0102 - acc: 0.7576\n",
      "Epoch 407/500\n",
      "33/33 [==============================] - 0s 172us/sample - loss: 1.0091 - acc: 0.7576\n",
      "Epoch 408/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 1.0073 - acc: 0.7576\n",
      "Epoch 409/500\n",
      "33/33 [==============================] - 0s 204us/sample - loss: 1.0049 - acc: 0.7576\n",
      "Epoch 410/500\n",
      "33/33 [==============================] - 0s 134us/sample - loss: 1.0006 - acc: 0.7273\n",
      "Epoch 411/500\n",
      "33/33 [==============================] - 0s 167us/sample - loss: 0.9959 - acc: 0.7273\n",
      "Epoch 412/500\n",
      "33/33 [==============================] - 0s 194us/sample - loss: 0.9919 - acc: 0.7273\n",
      "Epoch 413/500\n",
      "33/33 [==============================] - 0s 250us/sample - loss: 0.9888 - acc: 0.7273\n",
      "Epoch 414/500\n",
      "33/33 [==============================] - 0s 212us/sample - loss: 0.9867 - acc: 0.7273\n",
      "Epoch 415/500\n",
      "33/33 [==============================] - 0s 115us/sample - loss: 0.9851 - acc: 0.6970\n",
      "Epoch 416/500\n",
      "33/33 [==============================] - 0s 145us/sample - loss: 0.9839 - acc: 0.6667\n",
      "Epoch 417/500\n",
      "33/33 [==============================] - 0s 240us/sample - loss: 0.9824 - acc: 0.6667\n",
      "Epoch 418/500\n",
      "33/33 [==============================] - 0s 133us/sample - loss: 0.9800 - acc: 0.6667\n",
      "Epoch 419/500\n",
      "33/33 [==============================] - 0s 179us/sample - loss: 0.9756 - acc: 0.6667\n",
      "Epoch 420/500\n",
      "33/33 [==============================] - 0s 198us/sample - loss: 0.9711 - acc: 0.6667\n",
      "Epoch 421/500\n",
      "33/33 [==============================] - 0s 223us/sample - loss: 0.9665 - acc: 0.6970\n",
      "Epoch 422/500\n",
      "33/33 [==============================] - 0s 193us/sample - loss: 0.9624 - acc: 0.6970\n",
      "Epoch 423/500\n",
      "33/33 [==============================] - 0s 147us/sample - loss: 0.9594 - acc: 0.6970\n",
      "Epoch 424/500\n",
      "33/33 [==============================] - 0s 234us/sample - loss: 0.9575 - acc: 0.6667\n",
      "Epoch 425/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 0.9551 - acc: 0.7273\n",
      "Epoch 426/500\n",
      "33/33 [==============================] - 0s 202us/sample - loss: 0.9528 - acc: 0.7576\n",
      "Epoch 427/500\n",
      "33/33 [==============================] - 0s 175us/sample - loss: 0.9500 - acc: 0.7576\n",
      "Epoch 428/500\n",
      "33/33 [==============================] - 0s 201us/sample - loss: 0.9471 - acc: 0.7576\n",
      "Epoch 429/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 0.9447 - acc: 0.7273\n",
      "Epoch 430/500\n",
      "33/33 [==============================] - 0s 110us/sample - loss: 0.9415 - acc: 0.7273\n",
      "Epoch 431/500\n",
      "33/33 [==============================] - 0s 125us/sample - loss: 0.9382 - acc: 0.7273\n",
      "Epoch 432/500\n",
      "33/33 [==============================] - 0s 111us/sample - loss: 0.9354 - acc: 0.7273\n",
      "Epoch 433/500\n",
      "33/33 [==============================] - 0s 188us/sample - loss: 0.9330 - acc: 0.6970\n",
      "Epoch 434/500\n",
      "33/33 [==============================] - 0s 179us/sample - loss: 0.9313 - acc: 0.6667\n",
      "Epoch 435/500\n",
      "33/33 [==============================] - 0s 139us/sample - loss: 0.9314 - acc: 0.6667\n",
      "Epoch 436/500\n",
      "33/33 [==============================] - 0s 144us/sample - loss: 0.9325 - acc: 0.6667\n",
      "Epoch 437/500\n",
      "33/33 [==============================] - 0s 124us/sample - loss: 0.9303 - acc: 0.6364\n",
      "Epoch 438/500\n",
      "33/33 [==============================] - 0s 191us/sample - loss: 0.9251 - acc: 0.6667\n",
      "Epoch 439/500\n",
      "33/33 [==============================] - 0s 144us/sample - loss: 0.9207 - acc: 0.6667\n",
      "Epoch 440/500\n",
      "33/33 [==============================] - 0s 218us/sample - loss: 0.9173 - acc: 0.7273\n",
      "Epoch 441/500\n",
      "33/33 [==============================] - 0s 194us/sample - loss: 0.9144 - acc: 0.7576\n",
      "Epoch 442/500\n",
      "33/33 [==============================] - 0s 187us/sample - loss: 0.9122 - acc: 0.7576\n",
      "Epoch 443/500\n",
      "33/33 [==============================] - 0s 168us/sample - loss: 0.9101 - acc: 0.7879\n",
      "Epoch 444/500\n",
      "33/33 [==============================] - 0s 165us/sample - loss: 0.9094 - acc: 0.7879\n",
      "Epoch 445/500\n",
      "33/33 [==============================] - 0s 197us/sample - loss: 0.9083 - acc: 0.8182\n",
      "Epoch 446/500\n",
      "33/33 [==============================] - 0s 146us/sample - loss: 0.9076 - acc: 0.8182\n",
      "Epoch 447/500\n",
      "33/33 [==============================] - 0s 186us/sample - loss: 0.9082 - acc: 0.8182\n",
      "Epoch 448/500\n",
      "33/33 [==============================] - 0s 155us/sample - loss: 0.9098 - acc: 0.7576\n",
      "Epoch 449/500\n",
      "33/33 [==============================] - 0s 186us/sample - loss: 0.9092 - acc: 0.6667\n",
      "Epoch 450/500\n",
      "33/33 [==============================] - 0s 120us/sample - loss: 0.9066 - acc: 0.6667\n",
      "Epoch 451/500\n",
      "33/33 [==============================] - 0s 138us/sample - loss: 0.9031 - acc: 0.6667\n",
      "Epoch 452/500\n",
      "33/33 [==============================] - 0s 251us/sample - loss: 0.8976 - acc: 0.6667\n",
      "Epoch 453/500\n",
      "33/33 [==============================] - 0s 166us/sample - loss: 0.8925 - acc: 0.6667\n",
      "Epoch 454/500\n",
      "33/33 [==============================] - 0s 177us/sample - loss: 0.8881 - acc: 0.6667\n",
      "Epoch 455/500\n",
      "33/33 [==============================] - 0s 158us/sample - loss: 0.8846 - acc: 0.6667\n",
      "Epoch 456/500\n",
      "33/33 [==============================] - 0s 172us/sample - loss: 0.8818 - acc: 0.6667\n",
      "Epoch 457/500\n",
      "33/33 [==============================] - 0s 227us/sample - loss: 0.8798 - acc: 0.7273\n",
      "Epoch 458/500\n",
      "33/33 [==============================] - 0s 129us/sample - loss: 0.8767 - acc: 0.7273\n",
      "Epoch 459/500\n",
      "33/33 [==============================] - 0s 201us/sample - loss: 0.8743 - acc: 0.7273\n",
      "Epoch 460/500\n",
      "33/33 [==============================] - 0s 105us/sample - loss: 0.8712 - acc: 0.7273\n",
      "Epoch 461/500\n",
      "33/33 [==============================] - 0s 232us/sample - loss: 0.8684 - acc: 0.6970\n",
      "Epoch 462/500\n",
      "33/33 [==============================] - 0s 150us/sample - loss: 0.8666 - acc: 0.6970\n",
      "Epoch 463/500\n",
      "33/33 [==============================] - 0s 185us/sample - loss: 0.8650 - acc: 0.6667\n",
      "Epoch 464/500\n",
      "33/33 [==============================] - 0s 139us/sample - loss: 0.8628 - acc: 0.6667\n",
      "Epoch 465/500\n",
      "33/33 [==============================] - 0s 142us/sample - loss: 0.8613 - acc: 0.6970\n",
      "Epoch 466/500\n",
      "33/33 [==============================] - 0s 278us/sample - loss: 0.8606 - acc: 0.7273\n",
      "Epoch 467/500\n",
      "33/33 [==============================] - 0s 151us/sample - loss: 0.8591 - acc: 0.7576\n",
      "Epoch 468/500\n",
      "33/33 [==============================] - 0s 97us/sample - loss: 0.8566 - acc: 0.7576\n",
      "Epoch 469/500\n",
      "33/33 [==============================] - 0s 164us/sample - loss: 0.8531 - acc: 0.8182\n",
      "Epoch 470/500\n",
      "33/33 [==============================] - 0s 250us/sample - loss: 0.8490 - acc: 0.8182\n",
      "Epoch 471/500\n",
      "33/33 [==============================] - 0s 233us/sample - loss: 0.8459 - acc: 0.8182\n",
      "Epoch 472/500\n",
      "33/33 [==============================] - 0s 174us/sample - loss: 0.8436 - acc: 0.8182\n",
      "Epoch 473/500\n",
      "33/33 [==============================] - 0s 259us/sample - loss: 0.8414 - acc: 0.8182\n",
      "Epoch 474/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.8610 - acc: 0.812 - 0s 143us/sample - loss: 0.8389 - acc: 0.8182\n",
      "Epoch 475/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 139us/sample - loss: 0.8367 - acc: 0.8182\n",
      "Epoch 476/500\n",
      "33/33 [==============================] - 0s 173us/sample - loss: 0.8342 - acc: 0.8182\n",
      "Epoch 477/500\n",
      "33/33 [==============================] - 0s 176us/sample - loss: 0.8316 - acc: 0.8182\n",
      "Epoch 478/500\n",
      "33/33 [==============================] - 0s 248us/sample - loss: 0.8290 - acc: 0.8182\n",
      "Epoch 479/500\n",
      "33/33 [==============================] - 0s 146us/sample - loss: 0.8265 - acc: 0.7879\n",
      "Epoch 480/500\n",
      "33/33 [==============================] - 0s 273us/sample - loss: 0.8239 - acc: 0.7879\n",
      "Epoch 481/500\n",
      "33/33 [==============================] - 0s 331us/sample - loss: 0.8209 - acc: 0.7879\n",
      "Epoch 482/500\n",
      "33/33 [==============================] - 0s 155us/sample - loss: 0.8178 - acc: 0.7879\n",
      "Epoch 483/500\n",
      "33/33 [==============================] - 0s 276us/sample - loss: 0.8152 - acc: 0.7576\n",
      "Epoch 484/500\n",
      "33/33 [==============================] - 0s 168us/sample - loss: 0.8132 - acc: 0.7576\n",
      "Epoch 485/500\n",
      "33/33 [==============================] - 0s 242us/sample - loss: 0.8114 - acc: 0.7273\n",
      "Epoch 486/500\n",
      "33/33 [==============================] - 0s 205us/sample - loss: 0.8099 - acc: 0.7273\n",
      "Epoch 487/500\n",
      "33/33 [==============================] - 0s 148us/sample - loss: 0.8072 - acc: 0.7273\n",
      "Epoch 488/500\n",
      "33/33 [==============================] - 0s 133us/sample - loss: 0.8043 - acc: 0.7576\n",
      "Epoch 489/500\n",
      "33/33 [==============================] - 0s 214us/sample - loss: 0.8016 - acc: 0.7576\n",
      "Epoch 490/500\n",
      "33/33 [==============================] - 0s 206us/sample - loss: 0.7987 - acc: 0.7879\n",
      "Epoch 491/500\n",
      "33/33 [==============================] - 0s 203us/sample - loss: 0.7960 - acc: 0.7879\n",
      "Epoch 492/500\n",
      "33/33 [==============================] - 0s 232us/sample - loss: 0.7930 - acc: 0.7879\n",
      "Epoch 493/500\n",
      "33/33 [==============================] - 0s 152us/sample - loss: 0.7902 - acc: 0.7879\n",
      "Epoch 494/500\n",
      "33/33 [==============================] - 0s 133us/sample - loss: 0.7876 - acc: 0.7879\n",
      "Epoch 495/500\n",
      "33/33 [==============================] - 0s 123us/sample - loss: 0.7854 - acc: 0.7879\n",
      "Epoch 496/500\n",
      "33/33 [==============================] - 0s 170us/sample - loss: 0.7832 - acc: 0.7879\n",
      "Epoch 497/500\n",
      "33/33 [==============================] - 0s 207us/sample - loss: 0.7808 - acc: 0.7879\n",
      "Epoch 498/500\n",
      "33/33 [==============================] - 0s 261us/sample - loss: 0.7774 - acc: 0.7576\n",
      "Epoch 499/500\n",
      "33/33 [==============================] - 0s 194us/sample - loss: 0.7739 - acc: 0.7576\n",
      "Epoch 500/500\n",
      "33/33 [==============================] - 0s 220us/sample - loss: 0.7708 - acc: 0.7879\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spatial-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"chat_model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sophisticated-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "executive-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import colorama \n",
    "colorama.init()\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "with open(\"text.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "infinite-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    # load trained model\n",
    "    model = keras.models.load_model('chat_model')\n",
    "\n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "\n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    \n",
    "    while True:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        inp = input()\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
    "                                             truncating='post', maxlen=max_len))\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        for i in data['intents']:\n",
    "            if i['tag'] == tag:\n",
    "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
    "\n",
    "        # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "close-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start messaging with the bot (type quit to stop)!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-24212bb6ad87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYELLOW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Start messaging with the bot (type quit to stop)!\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mStyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRESET_ALL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-dec5574915ab>\u001b[0m in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# load trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chat_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# load tokenizer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    144\u001b[0m       h5py is not None and (\n\u001b[0;32m    145\u001b[0m           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\Anaconda3\\envs\\nlp_new\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m    212\u001b[0m                                                custom_objects=custom_objects)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sunset-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chat.util import Chat, reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "southeast-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\n",
    "        r\"(.*)my name is (.*)\",\n",
    "        [\"Hello %2, How are you today ?\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*)help(.*) \",\n",
    "        [\"I can help you \",]\n",
    "    ],\n",
    "     [\n",
    "        r\"(.*) your name ?\",\n",
    "        [\"My name is thecleverprogrammer, but you can just call me robot and I'm a chatbot .\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"how are you (.*) ?\",\n",
    "        [\"I'm doing very well\", \"i am great !\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"sorry (.*)\",\n",
    "        [\"Its alright\",\"Its OK, never mind that\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"i'm (.*) (good|well|okay|ok)\",\n",
    "        [\"Nice to hear that\",\"Alright, great !\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(hi|hey|hello|hola|holla)(.*)\",\n",
    "        [\"Hello\", \"Hey there\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"what (.*) want ?\",\n",
    "        [\"Make me an offer I can't refuse\",]\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        r\"(.*)created(.*)\",\n",
    "        [\"Aman Kharwal created me using Python's NLTK library \",\"top secret ;)\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*) (location|city) ?\",\n",
    "        ['New Delhi, India',]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*)raining in (.*)\",\n",
    "        [\"No rain in the past 4 days here in %2\",\"In %2 there is a 50% chance of rain\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"how (.*) health (.*)\",\n",
    "        [\"Health is very important, but I am a computer, so I don't need to worry about my health \",]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*)(sports|game|sport)(.*)\",\n",
    "        [\"I'm a very big fan of Cricket\",]\n",
    "    ],\n",
    "    [\n",
    "        r\"who (.*) (Cricketer|Batsman)?\",\n",
    "        [\"Virat Kohli\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"quit\",\n",
    "        [\"Bye for now. See you soon :) \",\"It was nice talking to you. See you soon :)\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"(.*)\",\n",
    "        ['That is nice to hear']\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "color-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i am': 'you are', 'i was': 'you were', 'i': 'you', \"i'm\": 'you are', \"i'd\": 'you would', \"i've\": 'you have', \"i'll\": 'you will', 'my': 'your', 'you are': 'I am', 'you were': 'I was', \"you've\": 'I have', \"you'll\": 'I will', 'your': 'my', 'yours': 'mine', 'you': 'me', 'me': 'you'}\n"
     ]
    }
   ],
   "source": [
    "print(reflections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "rotary-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dummy_reflections= {\n",
    "    \"go\"     : \"gone\",\n",
    "    \"hello\"    : \"hey there\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "welcome-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(pairs, reflections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "documented-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Hi\n",
      "Hey there\n",
      ">what are you doing\n",
      "That is nice to hear\n",
      ">ohh awesome\n",
      "That is nice to hear\n",
      ">why\n",
      "That is nice to hear\n",
      ">ohoo\n",
      "That is nice to hear\n",
      ">tell me about yourself\n",
      "That is nice to hear\n",
      ">bj\n",
      "That is nice to hear\n",
      ">n\n",
      "That is nice to hear\n",
      "> \n",
      "That is nice to hear\n",
      ">do you like cricket\n",
      "That is nice to hear\n",
      ">quit\n",
      "Bye for now. See you soon :) \n"
     ]
    }
   ],
   "source": [
    "chat.converse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-belly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'nlp_new",
   "language": "python",
   "name": "nlp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
